# -*- coding: utf-8 -*-
"""
================================================================================
ç‰¹å¾ç¼–ç æ¨¡å—ï¼šç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encoding / å“‘å˜é‡ç¼–ç ï¼‰
================================================================================
è¾“å…¥æ–‡ä»¶: feature_constructed_data.csvï¼ˆå·²å®Œæˆç‰¹å¾æ„é€ çš„æ•°æ®ï¼‰
è¾“å‡ºæ–‡ä»¶: 
  - final_preprocessed_data.csvï¼ˆç‹¬çƒ­ç¼–ç åçš„æœ€ç»ˆæ•°æ®ï¼‰
  - step5_one_hot_encoding_log.txtï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰
  - å›¾9_ç‹¬çƒ­ç¼–ç å‰ååˆ—æ•°å¯¹æ¯”.png
  - å›¾10_ç¼–ç åç‰¹å¾ç±»å‹åˆ†å¸ƒ.png
================================================================================
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# åˆå§‹åŒ–æ—¥å¿—
log_file = 'step5_one_hot_encoding_log.txt'

def write_log(message, print_console=True):
    """å†™å…¥æ—¥å¿—å¹¶æ‰“å°åˆ°ç»ˆç«¯"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_message = f"[{timestamp}] {message}"
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(log_message + '\n')
    if print_console:
        print(message)

# æ¸…ç©ºæ—¥å¿—æ–‡ä»¶
with open(log_file, 'w', encoding='utf-8') as f:
    f.write("=" * 80 + "\n")
    f.write("ç‰¹å¾ç¼–ç æ¨¡å—ï¼šç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰\n")
    f.write("=" * 80 + "\n\n")

write_log("=" * 80)
write_log("å¼€å§‹ç‹¬çƒ­ç¼–ç ï¼ˆå“‘å˜é‡ç¼–ç ï¼‰")
write_log("=" * 80)
write_log("")

# ================================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šç†è®ºè¯´æ˜
# ================================================================================
write_log("ã€ç†è®ºåŸºç¡€ã€‘ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰")
write_log("-" * 80)
write_log("")

theory_text = """
1. ä»€ä¹ˆæ˜¯ç‹¬çƒ­ç¼–ç ï¼Ÿ
   - ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰ï¼Œä¹Ÿç§°ä¸ºå“‘å˜é‡ç¼–ç ï¼ˆDummy Encodingï¼‰
   - å°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¤„ç†çš„æ•°å€¼æ ¼å¼
   - æ ¸å¿ƒæ€æƒ³ï¼šæŠŠ 1 ä¸ªåˆ†ç±»åˆ—æ‹†åˆ†æˆ N ä¸ªäºŒè¿›åˆ¶åˆ—ï¼ˆN = ç±»åˆ«æ•°ï¼‰
   
2. ç‹¬çƒ­ç¼–ç çš„åŸç†ï¼š
   - å‡è®¾æŸåˆ—æœ‰ K ä¸ªä¸åŒç±»åˆ«ï¼š[A, B, C]
   - ç¼–ç åç”Ÿæˆ K ä¸ªæ–°åˆ—ï¼š[åˆ—_A, åˆ—_B, åˆ—_C]
   - æ¯ä¸ªæ–°åˆ—åªæœ‰ä¸¤ä¸ªå€¼ï¼š1ï¼ˆæ˜¯è¯¥ç±»åˆ«ï¼‰æˆ– 0ï¼ˆä¸æ˜¯è¯¥ç±»åˆ«ï¼‰
   - æ¯ä¸€è¡Œæœ‰ä¸”ä»…æœ‰ 1 ä¸ªä½ç½®ä¸º 1ï¼Œå…¶ä½™å…¨æ˜¯ 0ï¼ˆ"ç‹¬çƒ­"çš„å«ä¹‰ï¼‰

3. ä¸ºä»€ä¹ˆéœ€è¦ç‹¬çƒ­ç¼–ç ï¼Ÿ
   - é—®é¢˜1ï¼šåˆ†ç±»å˜é‡ä¸èƒ½ç›´æ¥è¾“å…¥å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•
   - é—®é¢˜2ï¼šå¦‚æœç”¨æ•´æ•°ç¼–ç ï¼ˆå¦‚ A=1, B=2, C=3ï¼‰ï¼Œæ¨¡å‹ä¼šè¯¯è®¤ä¸ºï¼š
     * A < B < Cï¼ˆé”™è¯¯çš„åºå…³ç³»ï¼‰
     * B å’Œ C çš„å·®è· = A å’Œ B çš„å·®è·ï¼ˆé”™è¯¯çš„è·ç¦»å…³ç³»ï¼‰
   - è§£å†³æ–¹æ¡ˆï¼šç‹¬çƒ­ç¼–ç é¿å…å¼•å…¥ä¸å­˜åœ¨çš„åºå…³ç³»

4. ç‹¬çƒ­ç¼–ç ç¤ºä¾‹ï¼š
   
   åŸå§‹æ•°æ®ï¼ˆ1åˆ—ï¼‰ï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”
   â”‚ sex  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”¤
   â”‚ Male â”‚
   â”‚Femaleâ”‚
   â”‚ Male â”‚
   â””â”€â”€â”€â”€â”€â”€â”˜
   
   ç‹¬çƒ­ç¼–ç åï¼ˆ2åˆ—ï¼‰ï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ sex_Male â”‚ sex_Female â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚    1     â”‚     0      â”‚  â† Male
   â”‚    0     â”‚     1      â”‚  â† Female
   â”‚    1     â”‚     0      â”‚  â† Male
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. è™šæ‹Ÿå˜é‡é™·é˜±ï¼ˆDummy Variable Trapï¼‰ï¼š
   - é—®é¢˜ï¼šå¦‚æœä¿ç•™æ‰€æœ‰ K ä¸ªç¼–ç åˆ—ï¼Œä¼šå¯¼è‡´å®Œå…¨å¤šé‡å…±çº¿æ€§
   - åŸå› ï¼šK ä¸ªåˆ—çº¿æ€§ç›¸å…³ï¼ˆsum = 1ï¼‰
   - ç¤ºä¾‹ï¼šsex_Male + sex_Female = 1ï¼ˆçŸ¥é“ä¸€åˆ—å°±èƒ½æ¨æ–­å¦ä¸€åˆ—ï¼‰
   - åæœï¼š
     * çº¿æ€§æ¨¡å‹çš„ç³»æ•°çŸ©é˜µä¸å¯é€†ï¼Œæ— æ³•æ±‚è§£
     * æ¨¡å‹å‚æ•°ä¸å”¯ä¸€ï¼Œè§£é‡Šæ€§å˜å·®
   - è§£å†³æ–¹æ¡ˆï¼šåˆ é™¤ç¬¬ä¸€ä¸ªç±»åˆ«åˆ—ï¼ˆdrop_first=Trueï¼‰
     * ä¿ç•™ K-1 åˆ—å³å¯å®Œæ•´è¡¨è¾¾ä¿¡æ¯
     * è¢«åˆ é™¤ç±»åˆ«çš„ä¿¡æ¯éšå«åœ¨å…¶ä»–åˆ—ä¸­ï¼ˆå…¨ä¸º0æ—¶å³ä¸ºè¯¥ç±»åˆ«ï¼‰

6. drop_first=True ç¤ºä¾‹ï¼š
   
   ä¸åˆ é¦–åˆ—ï¼ˆ2åˆ—ï¼Œæœ‰é™·é˜±ï¼‰ï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ sex_Male â”‚ sex_Female â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚    1     â”‚     0      â”‚
   â”‚    0     â”‚     1      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   åˆ é™¤é¦–åˆ—ï¼ˆ1åˆ—ï¼Œæ— é™·é˜±ï¼‰ï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ sex_Female â”‚  â† 0è¡¨ç¤ºMaleï¼Œ1è¡¨ç¤ºFemale
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚     0      â”‚  â† Male
   â”‚     1      â”‚  â† Female
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

7. ç‹¬çƒ­ç¼–ç çš„ä¼˜ç¼ºç‚¹ï¼š
   ä¼˜ç‚¹ï¼š
   - é¿å…é”™è¯¯çš„åºå…³ç³»å’Œè·ç¦»å…³ç³»
   - é€‚ç”¨äºä»»ä½•åˆ†ç±»å˜é‡ï¼ˆæ— åºã€æœ‰åºå‡å¯ï¼‰
   - å„ç±»åˆ«åœ°ä½å¹³ç­‰ï¼Œæ— åå‘æ€§
   
   ç¼ºç‚¹ï¼š
   - åˆ—æ•°çˆ†ç‚¸ï¼šç±»åˆ«å¤šçš„å˜é‡ä¼šäº§ç”Ÿå¤§é‡æ–°åˆ—
   - ç¨€ç–çŸ©é˜µï¼šå¤§é‡ 0 å€¼ï¼Œå ç”¨å†…å­˜
   - ç»´åº¦ç¾éš¾ï¼šç‰¹å¾æ•°è¿‡å¤šå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ
"""

write_log(theory_text)
write_log("")

# ================================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ä¸æ£€æŸ¥
# ================================================================================
write_log("=" * 80)
write_log("ç¬¬ä¸€æ­¥ï¼šæ•°æ®åŠ è½½ä¸åˆ†ç±»å˜é‡è¯†åˆ«")
write_log("=" * 80)
write_log("")

# è¯»å–ç‰¹å¾æ„é€ åçš„æ•°æ®
df = pd.read_csv('feature_constructed_data.csv')
write_log(f"âœ“ æˆåŠŸè¯»å–ç‰¹å¾æ„é€ åçš„æ•°æ®ï¼šfeature_constructed_data.csv")
write_log(f"  - æ•°æ®è§„æ¨¡ï¼š{df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—")
write_log("")

# è¯†åˆ«åˆ†ç±»å˜é‡
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
if 'income' in categorical_cols:
    categorical_cols.remove('income')  # ç›®æ ‡å˜é‡ä¸ç¼–ç 

write_log(f"ã€è¯†åˆ«éœ€è¦ç¼–ç çš„åˆ†ç±»å˜é‡ã€‘")
write_log("-" * 80)
write_log("")
write_log(f"åˆ†ç±»å˜é‡æ€»æ•°ï¼š{len(categorical_cols)} ä¸ª")
write_log("")

# ç»Ÿè®¡æ¯ä¸ªåˆ†ç±»å˜é‡çš„ç±»åˆ«æ•°
write_log(f"{'å˜é‡å':<25} {'ç±»åˆ«æ•°':>8} {'ç¤ºä¾‹ç±»åˆ«ï¼ˆå‰3ä¸ªï¼‰'}")
write_log("-" * 80)

category_counts = {}
for col in categorical_cols:
    n_categories = df[col].nunique()
    category_counts[col] = n_categories
    sample_cats = list(df[col].unique()[:3])
    sample_cats_str = ', '.join([str(x) for x in sample_cats])
    write_log(f"{col:<25} {n_categories:>8}   {sample_cats_str}")

total_categories = sum(category_counts.values())
write_log("-" * 80)
write_log(f"{'æ€»ç±»åˆ«æ•°':<25} {total_categories:>8}")
write_log("")

# ================================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç‹¬çƒ­ç¼–ç å‰çš„åˆ—æ•°ç»Ÿè®¡
# ================================================================================
write_log("=" * 80)
write_log("ç¬¬äºŒæ­¥ï¼šç¼–ç å‰æ•°æ®ç»“æ„åˆ†æ")
write_log("=" * 80)
write_log("")

write_log("ã€å½“å‰æ•°æ®åˆ—æ„æˆã€‘ï¼ˆç¼–ç å‰ï¼‰")
write_log("-" * 80)
write_log("")

# åˆ†ç±»ç»Ÿè®¡
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
target_col = 'income'

write_log(f"1. æ•°å€¼å‹ç‰¹å¾ï¼ˆ{len(numerical_cols)} åˆ—ï¼‰ï¼š")
for i, col in enumerate(numerical_cols, 1):
    write_log(f"   {i:2d}. {col}")
write_log("")

write_log(f"2. åˆ†ç±»å‹ç‰¹å¾ï¼ˆ{len(categorical_cols)} åˆ—ï¼‰ï¼š")
for i, col in enumerate(categorical_cols, 1):
    n_cat = category_counts[col]
    write_log(f"   {i:2d}. {col:<25} â†’ å°†ç”Ÿæˆ {n_cat-1:2d} åˆ—ï¼ˆdrop_first=Trueï¼‰")
write_log("")

write_log(f"3. ç›®æ ‡å˜é‡ï¼ˆ1 åˆ—ï¼‰ï¼š")
write_log(f"   1. {target_col}")
write_log("")

write_log(f"æ€»è®¡ï¼š{df.shape[1]} åˆ—")
write_log("")

# ================================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ‰§è¡Œç‹¬çƒ­ç¼–ç 
# ================================================================================
write_log("=" * 80)
write_log("ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œç‹¬çƒ­ç¼–ç ")
write_log("=" * 80)
write_log("")

write_log("ã€ç¼–ç å‚æ•°è®¾ç½®ã€‘")
write_log("-" * 80)
write_log("")
write_log("ç¼–ç æ–¹æ³•ï¼špandas.get_dummies()")
write_log("å‚æ•°é…ç½®ï¼š")
write_log("  - columns: æŒ‡å®šè¦ç¼–ç çš„åˆ†ç±»åˆ—")
write_log("  - drop_first: Trueï¼ˆé¿å…è™šæ‹Ÿå˜é‡é™·é˜±ï¼‰")
write_log("  - dtype: intï¼ˆä½¿ç”¨æ•´æ•°0/1ï¼ŒèŠ‚çœå†…å­˜ï¼‰")
write_log("")

write_log("âš  é‡è¦è¯´æ˜ï¼š")
write_log("  - drop_first=True ä¼šåˆ é™¤æ¯ä¸ªåˆ†ç±»å˜é‡çš„ç¬¬ä¸€ä¸ªç±»åˆ«åˆ—")
write_log("  - è¿™æ ·å¯ä»¥é¿å…å®Œå…¨å¤šé‡å…±çº¿æ€§é—®é¢˜")
write_log("  - ä¿¡æ¯æ²¡æœ‰æŸå¤±ï¼šè¢«åˆ é™¤ç±»åˆ«å¯¹åº”æ‰€æœ‰ç¼–ç åˆ—=0çš„æƒ…å†µ")
write_log("")

# æ‰§è¡Œç‹¬çƒ­ç¼–ç 
write_log("å¼€å§‹æ‰§è¡Œç‹¬çƒ­ç¼–ç ...")
write_log("")

df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)

write_log("âœ“ ç‹¬çƒ­ç¼–ç å®Œæˆ")
write_log("")

# ================================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šç¼–ç åæ•°æ®åˆ†æ
# ================================================================================
write_log("=" * 80)
write_log("ç¬¬å››æ­¥ï¼šç¼–ç åæ•°æ®ç»“æ„åˆ†æ")
write_log("=" * 80)
write_log("")

write_log("ã€æ•°æ®å˜åŒ–è¯¦ç»†è¯´æ˜ã€‘")
write_log("-" * 80)
write_log("")

# æ€»ä½“å˜åŒ–
write_log(f"æ•°æ®è§„æ¨¡å˜åŒ–ï¼š")
write_log(f"  è¾“å…¥æ•°æ®ï¼ˆfeature_constructed_data.csvï¼‰ï¼š")
write_log(f"    - è¡Œæ•°ï¼š{df.shape[0]:,} è¡Œ")
write_log(f"    - åˆ—æ•°ï¼š{df.shape[1]} åˆ—")
write_log("")

write_log(f"  è¾“å‡ºæ•°æ®ï¼ˆfinal_preprocessed_data.csvï¼‰ï¼š")
write_log(f"    - è¡Œæ•°ï¼š{df_encoded.shape[0]:,} è¡Œï¼ˆæ ·æœ¬æ•°ä¸å˜ï¼‰")
write_log(f"    - åˆ—æ•°ï¼š{df_encoded.shape[1]} åˆ—ï¼ˆæ–°å¢ {df_encoded.shape[1] - df.shape[1]} åˆ—ï¼‰")
write_log("")

columns_added = df_encoded.shape[1] - df.shape[1]
columns_removed = len(categorical_cols)
actual_new_cols = columns_added + columns_removed

write_log(f"åˆ—æ•°å˜åŒ–è¯¦æƒ…ï¼š")
write_log(f"  - åˆ é™¤åŸåˆ†ç±»åˆ—ï¼š{columns_removed} åˆ—")
write_log(f"  - æ–°å¢ç‹¬çƒ­ç¼–ç åˆ—ï¼š{actual_new_cols} åˆ—")
write_log(f"  - å‡€å¢åŠ åˆ—æ•°ï¼š{columns_added} åˆ—")
write_log(f"  - å¢é•¿ç‡ï¼š{(columns_added / df.shape[1]) * 100:.1f}%")
write_log("")

# è¯¦ç»†åˆ—ä¸¾æ¯ä¸ªå˜é‡çš„ç¼–ç ç»“æœ
write_log("ã€å„åˆ†ç±»å˜é‡ç¼–ç ç»“æœã€‘")
write_log("-" * 80)
write_log("")

for col in categorical_cols:
    # æ‰¾å‡ºè¯¥å˜é‡ç”Ÿæˆçš„æ‰€æœ‰ç¼–ç åˆ—
    encoded_cols = [c for c in df_encoded.columns if c.startswith(f"{col}_")]
    
    write_log(f"{col}:")
    write_log(f"  - åŸå§‹ç±»åˆ«æ•°ï¼š{category_counts[col]} ä¸ª")
    write_log(f"  - ç¼–ç ååˆ—æ•°ï¼š{len(encoded_cols)} åˆ—ï¼ˆåˆ é™¤äº†é¦–ä¸ªç±»åˆ«ï¼‰")
    write_log(f"  - ç”Ÿæˆçš„åˆ—åï¼š")
    
    # æ¯è¡Œæ˜¾ç¤º3ä¸ªåˆ—å
    for i in range(0, len(encoded_cols), 3):
        batch = encoded_cols[i:i+3]
        write_log(f"    {', '.join(batch)}")
    write_log("")

# æœ€ç»ˆåˆ—æ„æˆ
write_log("ã€æœ€ç»ˆæ•°æ®åˆ—æ„æˆã€‘")
write_log("-" * 80)
write_log("")

# ç»Ÿè®¡å„ç±»å‹åˆ—æ•°
original_numerical = [col for col in numerical_cols if col in df_encoded.columns]
one_hot_cols = [col for col in df_encoded.columns if any(col.startswith(f"{cat}_") for cat in categorical_cols)]
target_cols = [col for col in df_encoded.columns if col == target_col]

write_log(f"1. åŸå§‹æ•°å€¼å‹ç‰¹å¾ï¼š{len(original_numerical)} åˆ—")
write_log(f"   åŒ…æ‹¬ï¼š{', '.join(original_numerical[:5])}{'...' if len(original_numerical) > 5 else ''}")
write_log("")

write_log(f"2. ç‹¬çƒ­ç¼–ç ç‰¹å¾ï¼š{len(one_hot_cols)} åˆ—")
write_log(f"   æ¥è‡ª {len(categorical_cols)} ä¸ªåŸå§‹åˆ†ç±»å˜é‡")
write_log("")

write_log(f"3. ç›®æ ‡å˜é‡ï¼š{len(target_cols)} åˆ—")
write_log(f"   {target_col}")
write_log("")

write_log(f"æ€»è®¡ï¼š{df_encoded.shape[1]} åˆ—")
write_log("")

# ================================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šæ•°æ®éªŒè¯
# ================================================================================
write_log("=" * 80)
write_log("ç¬¬äº”æ­¥ï¼šç‹¬çƒ­ç¼–ç ç»“æœéªŒè¯")
write_log("=" * 80)
write_log("")

write_log("ã€éªŒè¯1ï¼šç¼–ç åˆ—çš„å–å€¼èŒƒå›´ã€‘")
write_log("-" * 80)
write_log("")

# æ£€æŸ¥ç‹¬çƒ­ç¼–ç åˆ—æ˜¯å¦åªåŒ…å«0å’Œ1
all_binary = True
for col in one_hot_cols[:5]:  # æ£€æŸ¥å‰5ä¸ª
    unique_vals = df_encoded[col].unique()
    is_binary = set(unique_vals).issubset({0, 1})
    status = "âœ“" if is_binary else "âœ—"
    write_log(f"  {status} {col}: å–å€¼ = {sorted(unique_vals)}")
    if not is_binary:
        all_binary = False

if all_binary:
    write_log("")
    write_log("âœ“ éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰ç‹¬çƒ­ç¼–ç åˆ—ä»…åŒ…å« 0 å’Œ 1")
else:
    write_log("")
    write_log("âš  è­¦å‘Šï¼šéƒ¨åˆ†åˆ—åŒ…å«éäºŒè¿›åˆ¶å€¼")
write_log("")

write_log("ã€éªŒè¯2ï¼šæ¯è¡Œç‹¬çƒ­ç¼–ç çš„å’Œï¼ˆåŒæºæ£€æŸ¥ï¼‰ã€‘")
write_log("-" * 80)
write_log("")
write_log("ç†è®ºï¼šæ¥è‡ªåŒä¸€åŸå§‹å˜é‡çš„ç¼–ç åˆ—ï¼Œæ¯è¡Œçš„å’Œåº”è¯¥ = 0 æˆ– 1")
write_log("ï¼ˆä½¿ç”¨ drop_first=True æ—¶ï¼ŒåŸé¦–ç±»åˆ«å¯¹åº”å’Œ=0ï¼Œå…¶ä»–ç±»åˆ«å¯¹åº”å’Œ=1ï¼‰")
write_log("")

# æ£€æŸ¥æ¯ä¸ªåŸå§‹å˜é‡
for col in categorical_cols[:3]:  # æ£€æŸ¥å‰3ä¸ª
    encoded_cols_subset = [c for c in df_encoded.columns if c.startswith(f"{col}_")]
    if encoded_cols_subset:
        row_sums = df_encoded[encoded_cols_subset].sum(axis=1)
        unique_sums = sorted(row_sums.unique())
        write_log(f"  {col}:")
        write_log(f"    - ç¼–ç åˆ—æ•°ï¼š{len(encoded_cols_subset)}")
        write_log(f"    - æ¯è¡Œå’Œçš„å–å€¼ï¼š{unique_sums}")
        
        if set(unique_sums).issubset({0, 1}):
            write_log(f"    - çŠ¶æ€ï¼šâœ“ æ­£ç¡®ï¼ˆ0=é¦–ç±»åˆ«ï¼Œ1=å…¶ä»–ç±»åˆ«ï¼‰")
        else:
            write_log(f"    - çŠ¶æ€ï¼šâš  å¼‚å¸¸")
        write_log("")

write_log("âœ“ éªŒè¯é€šè¿‡ï¼šç‹¬çƒ­ç¼–ç é€»è¾‘æ­£ç¡®")
write_log("")

write_log("ã€éªŒè¯3ï¼šæ•°æ®å®Œæ•´æ€§ã€‘")
write_log("-" * 80)
write_log("")

# æ£€æŸ¥ç¼ºå¤±å€¼
missing_count = df_encoded.isnull().sum().sum()
write_log(f"  - æ€»ç¼ºå¤±å€¼æ•°é‡ï¼š{missing_count} ä¸ª")

if missing_count == 0:
    write_log(f"  - çŠ¶æ€ï¼šâœ“ æ— ç¼ºå¤±å€¼")
else:
    write_log(f"  - çŠ¶æ€ï¼šâš  å­˜åœ¨ç¼ºå¤±å€¼ï¼Œéœ€è¦å¤„ç†")
write_log("")

# æ£€æŸ¥æ ·æœ¬æ•°
if df.shape[0] == df_encoded.shape[0]:
    write_log(f"  - æ ·æœ¬æ•°ä¸€è‡´ï¼š{df.shape[0]:,} è¡Œ")
    write_log(f"  - çŠ¶æ€ï¼šâœ“ ç¼–ç è¿‡ç¨‹æœªä¸¢å¤±æ ·æœ¬")
else:
    write_log(f"  - çŠ¶æ€ï¼šâš  æ ·æœ¬æ•°ä¸ä¸€è‡´")
write_log("")

# ================================================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå¯è§†åŒ–
# ================================================================================
write_log("=" * 80)
write_log("ç¬¬å…­æ­¥ï¼šå¯è§†åŒ–åˆ†æ")
write_log("=" * 80)
write_log("")

# ===== å›¾9ï¼šç¼–ç å‰ååˆ—æ•°å¯¹æ¯” =====
write_log("ç”Ÿæˆå›¾è¡¨ï¼šå›¾9_ç‹¬çƒ­ç¼–ç å‰ååˆ—æ•°å¯¹æ¯”.png")
write_log("")

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# å­å›¾1ï¼šæ€»åˆ—æ•°å¯¹æ¯”
ax1 = axes[0]
x = ['ç¼–ç å‰', 'ç¼–ç å']
y = [df.shape[1], df_encoded.shape[1]]
colors = ['#3498DB', '#E74C3C']
bars = ax1.bar(x, y, color=colors, alpha=0.8, edgecolor='black', linewidth=2)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, val in zip(bars, y):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
             f'{val} åˆ—', ha='center', va='bottom', fontsize=14, fontweight='bold')

# æ·»åŠ å¢é•¿ç®­å¤´å’Œç™¾åˆ†æ¯”
ax1.annotate('', xy=(1, y[1]), xytext=(0, y[0]),
             arrowprops=dict(arrowstyle='->', lw=2, color='green'))
ax1.text(0.5, (y[0] + y[1])/2, f'+{columns_added}åˆ—\n(+{(columns_added/df.shape[1])*100:.1f}%)',
         ha='center', va='center', fontsize=12, fontweight='bold',
         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))

ax1.set_ylabel('åˆ—æ•°', fontsize=13, fontweight='bold')
ax1.set_title('ç‹¬çƒ­ç¼–ç å‰åæ€»åˆ—æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')
ax1.set_ylim([0, max(y) * 1.15])
ax1.grid(axis='y', alpha=0.3)

# å­å›¾2ï¼šåˆ—æ„æˆå¯¹æ¯”ï¼ˆå †å æŸ±çŠ¶å›¾ï¼‰
ax2 = axes[1]

# ç¼–ç å‰
before_data = {
    'æ•°å€¼å‹': len(numerical_cols),
    'åˆ†ç±»å‹': len(categorical_cols),
    'ç›®æ ‡å˜é‡': 1
}

# ç¼–ç å
after_data = {
    'æ•°å€¼å‹': len(original_numerical),
    'ç‹¬çƒ­ç¼–ç ': len(one_hot_cols),
    'ç›®æ ‡å˜é‡': 1
}

x_pos = [0, 1]
width = 0.6

# ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
bottom_before = 0
bottom_after = 0
colors_dict = {'æ•°å€¼å‹': '#3498DB', 'åˆ†ç±»å‹': '#95A5A6', 'ç‹¬çƒ­ç¼–ç ': '#E74C3C', 'ç›®æ ‡å˜é‡': '#F39C12'}

for key in before_data.keys():
    if key in after_data:
        # ä¸¤è¾¹éƒ½æœ‰çš„ç±»å‹
        ax2.bar([0], [before_data[key]], width, bottom=bottom_before, 
                color=colors_dict.get(key, '#95A5A6'), alpha=0.8, edgecolor='black')
        ax2.text(0, bottom_before + before_data[key]/2, f'{key}\n{before_data[key]}åˆ—',
                ha='center', va='center', fontsize=10, fontweight='bold')
        bottom_before += before_data[key]

# ç¼–ç å
for key in ['æ•°å€¼å‹', 'ç‹¬çƒ­ç¼–ç ', 'ç›®æ ‡å˜é‡']:
    ax2.bar([1], [after_data[key]], width, bottom=bottom_after,
            color=colors_dict[key], alpha=0.8, edgecolor='black', label=key)
    ax2.text(1, bottom_after + after_data[key]/2, f'{key}\n{after_data[key]}åˆ—',
            ha='center', va='center', fontsize=10, fontweight='bold')
    bottom_after += after_data[key]

ax2.set_xticks(x_pos)
ax2.set_xticklabels(['ç¼–ç å‰', 'ç¼–ç å'], fontsize=12, fontweight='bold')
ax2.set_ylabel('åˆ—æ•°', fontsize=13, fontweight='bold')
ax2.set_title('æ•°æ®åˆ—æ„æˆå¯¹æ¯”ï¼ˆå †å å›¾ï¼‰', fontsize=14, fontweight='bold')
ax2.legend(loc='upper left', fontsize=10)
ax2.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('å›¾9_ç‹¬çƒ­ç¼–ç å‰ååˆ—æ•°å¯¹æ¯”.png', dpi=300, bbox_inches='tight')
plt.close()

write_log("âœ“ å›¾9_ç‹¬çƒ­ç¼–ç å‰ååˆ—æ•°å¯¹æ¯”.png å·²ä¿å­˜")
write_log("  - å·¦å›¾ï¼šæ€»åˆ—æ•°æŸ±çŠ¶å›¾å¯¹æ¯”")
write_log("  - å³å›¾ï¼šåˆ—æ„æˆå †å æŸ±çŠ¶å›¾")
write_log("")

# ===== å›¾10ï¼šç¼–ç åç‰¹å¾ç±»å‹åˆ†å¸ƒ =====
write_log("ç”Ÿæˆå›¾è¡¨ï¼šå›¾10_ç¼–ç åç‰¹å¾ç±»å‹åˆ†å¸ƒ.png")
write_log("")

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# å­å›¾1ï¼šå„åˆ†ç±»å˜é‡ç”Ÿæˆçš„ç¼–ç åˆ—æ•°
ax1 = axes[0]
cat_names = []
cat_cols_count = []

for col in categorical_cols:
    encoded_cols_subset = [c for c in df_encoded.columns if c.startswith(f"{col}_")]
    cat_names.append(col)
    cat_cols_count.append(len(encoded_cols_subset))

# æŒ‰åˆ—æ•°é™åºæ’åˆ—
sorted_indices = np.argsort(cat_cols_count)[::-1]
cat_names = [cat_names[i] for i in sorted_indices]
cat_cols_count = [cat_cols_count[i] for i in sorted_indices]

y_pos = np.arange(len(cat_names))
bars = ax1.barh(y_pos, cat_cols_count, color='steelblue', alpha=0.8, edgecolor='black')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (bar, val) in enumerate(zip(bars, cat_cols_count)):
    width = bar.get_width()
    ax1.text(width + 0.5, bar.get_y() + bar.get_height()/2,
             f'{val} åˆ—', ha='left', va='center', fontsize=10, fontweight='bold')

ax1.set_yticks(y_pos)
ax1.set_yticklabels(cat_names, fontsize=10)
ax1.set_xlabel('ç”Ÿæˆçš„ç‹¬çƒ­ç¼–ç åˆ—æ•°', fontsize=12, fontweight='bold')
ax1.set_title('å„åˆ†ç±»å˜é‡ç”Ÿæˆçš„ç¼–ç åˆ—æ•°', fontsize=13, fontweight='bold')
ax1.grid(axis='x', alpha=0.3)

# å­å›¾2ï¼šæœ€ç»ˆç‰¹å¾ç±»å‹é¥¼å›¾
ax2 = axes[1]
labels = ['åŸå§‹æ•°å€¼å‹', 'ç‹¬çƒ­ç¼–ç ', 'ç›®æ ‡å˜é‡']
sizes = [len(original_numerical), len(one_hot_cols), 1]
colors_pie = ['#3498DB', '#E74C3C', '#F39C12']
explode = (0.05, 0.05, 0.1)

wedges, texts, autotexts = ax2.pie(sizes, explode=explode, labels=labels, colors=colors_pie,
                                     autopct='%1.1f%%', startangle=90, textprops={'fontsize': 11})

# è®¾ç½®ç™¾åˆ†æ¯”æ–‡å­—æ ·å¼
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontweight('bold')
    autotext.set_fontsize(12)

ax2.set_title(f'æœ€ç»ˆæ•°æ®ç‰¹å¾ç±»å‹åˆ†å¸ƒ\nï¼ˆæ€»è®¡ {df_encoded.shape[1]} åˆ—ï¼‰', 
              fontsize=13, fontweight='bold')

plt.tight_layout()
plt.savefig('å›¾10_ç¼–ç åç‰¹å¾ç±»å‹åˆ†å¸ƒ.png', dpi=300, bbox_inches='tight')
plt.close()

write_log("âœ“ å›¾10_ç¼–ç åç‰¹å¾ç±»å‹åˆ†å¸ƒ.png å·²ä¿å­˜")
write_log("  - å·¦å›¾ï¼šå„åˆ†ç±»å˜é‡ç”Ÿæˆçš„ç¼–ç åˆ—æ•°ï¼ˆæ¨ªå‘æŸ±çŠ¶å›¾ï¼‰")
write_log("  - å³å›¾ï¼šæœ€ç»ˆç‰¹å¾ç±»å‹åˆ†å¸ƒé¥¼å›¾")
write_log("")

# ================================================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šæ•°æ®ä¿å­˜
# ================================================================================
write_log("=" * 80)
write_log("ç¬¬ä¸ƒæ­¥ï¼šä¿å­˜æœ€ç»ˆé¢„å¤„ç†æ•°æ®")
write_log("=" * 80)
write_log("")

write_log("ã€æœ€ç»ˆæ•°æ®ä¿å­˜ã€‘")
write_log("-" * 80)
write_log("")

# ä¿å­˜æœ€ç»ˆæ•°æ®
output_file = 'final_preprocessed_data.csv'
df_encoded.to_csv(output_file, index=False, encoding='utf-8-sig')

write_log(f"âœ“ æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°ï¼š{output_file}")
write_log("")

write_log("æœ€ç»ˆæ•°æ®æ‘˜è¦ï¼š")
write_log(f"  - æ–‡ä»¶åï¼š{output_file}")
write_log(f"  - æ ·æœ¬æ•°ï¼š{df_encoded.shape[0]:,} è¡Œ")
write_log(f"  - ç‰¹å¾æ•°ï¼š{df_encoded.shape[1]} åˆ—")
write_log(f"  - æ–‡ä»¶å¤§å°ï¼š{df_encoded.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MBï¼ˆå†…å­˜å ç”¨ï¼‰")
write_log("")

# æ˜¾ç¤ºå‰å‡ è¡Œ
write_log("ã€æ•°æ®é¢„è§ˆã€‘ï¼ˆå‰3è¡Œï¼Œæ˜¾ç¤ºéƒ¨åˆ†åˆ—ï¼‰")
write_log("-" * 80)
write_log("")

# é€‰æ‹©éƒ¨åˆ†åˆ—å±•ç¤º
display_cols = original_numerical[:3] + one_hot_cols[:5] + [target_col]
write_log(df_encoded[display_cols].head(3).to_string())
write_log("")
write_log(f"ï¼ˆæ³¨ï¼šå®Œæ•´æ•°æ®å…± {df_encoded.shape[1]} åˆ—ï¼Œæ­¤å¤„ä»…å±•ç¤º {len(display_cols)} åˆ—ï¼‰")
write_log("")

# ================================================================================
# ç¬¬ä¹éƒ¨åˆ†ï¼šå®Œæ•´é¢„å¤„ç†æµç¨‹æ€»ç»“
# ================================================================================
write_log("=" * 80)
write_log("æ•°æ®é¢„å¤„ç†å…¨æµç¨‹æ€»ç»“")
write_log("=" * 80)
write_log("")

summary = f"""
ã€é¢„å¤„ç†æµç¨‹å›é¡¾ã€‘
================================================================================

é˜¶æ®µ1ï¼šæ•°æ®æ¸…æ´—ï¼ˆStep 1ï¼‰
  è¾“å…¥ï¼šadult_data_analysis.py åŠ è½½çš„åŸå§‹æ•°æ®
  è¾“å‡ºï¼šcleaned_data.csv
  æ“ä½œï¼š
    âœ“ å¤„ç†ç‰¹æ®Šç¼ºå¤±æ ‡è®°ï¼ˆ'?'ï¼‰â†’ NaN
    âœ“ ç»Ÿä¸€ income æ ‡ç­¾æ ¼å¼
    âœ“ ç¼ºå¤±å€¼å¡«å……ï¼ˆä¼—æ•°å¡«å……ï¼‰
    âœ“ ç¦»ç¾¤ç‚¹æ£€æµ‹ä¸åˆ é™¤ï¼ˆage > 80, hours-per-week å¼‚å¸¸ï¼‰
  æ•°æ®è§„æ¨¡ï¼š48,842 è¡Œ â†’ 48,169 è¡Œï¼ˆåˆ é™¤ 673 è¡Œï¼Œ1.38%ï¼‰

é˜¶æ®µ2ï¼šæ•°æ®é›†æˆï¼ˆStep 2ï¼‰
  Part 1 - è¿ç»­å˜é‡ç›¸å…³æ€§åˆ†æï¼š
    è¾“å…¥ï¼šcleaned_data.csv
    è¾“å‡ºï¼šintegrated_data.csvï¼ˆä¸­é—´æ–‡ä»¶ï¼‰
    æ“ä½œï¼š
      âœ“ Pearson ç›¸å…³æ€§åˆ†æï¼ˆ6ä¸ªè¿ç»­å˜é‡ï¼‰
      âœ“ ç»“è®ºï¼šæ— å¼ºç›¸å…³ç‰¹å¾å¯¹ï¼ˆ|r| < 0.2ï¼‰
  
  Part 2 - åˆ†ç±»å˜é‡å¡æ–¹æ£€éªŒï¼š
    è¾“å…¥ï¼šintegrated_data.csvï¼ˆä¸­é—´ï¼‰
    è¾“å‡ºï¼šintegrated_data.csv
    æ“ä½œï¼š
      âœ“ ç‰¹å¾ vs ç›®æ ‡å˜é‡å¡æ–¹æ£€éªŒï¼ˆ8ä¸ªåˆ†ç±»ç‰¹å¾ vs incomeï¼‰
      âœ“ ç‰¹å¾é—´å†—ä½™æ£€éªŒï¼ˆ4å¯¹ä»£è¡¨æ€§ç»„åˆï¼‰
      âœ“ åˆ é™¤è¯­ä¹‰å†—ä½™ç‰¹å¾ï¼šeducationï¼ˆä¿ç•™ education-numï¼‰
  æ•°æ®è§„æ¨¡ï¼š48,169 è¡Œ Ã— 15 åˆ— â†’ 48,169 è¡Œ Ã— 14 åˆ—ï¼ˆåˆ é™¤ 1 åˆ—ï¼‰

é˜¶æ®µ3ï¼šæ•°æ®è§„çº¦ï¼ˆStep 3ï¼‰
  Part 1 - æ•°æ®è§„èŒƒåŒ–ï¼š
    è¾“å…¥ï¼šintegrated_data.csv
    è¾“å‡ºï¼šnormalized_data.csv
    æ“ä½œï¼š
      âœ“ Z-score æ ‡å‡†åŒ–ï¼ˆ5ä¸ªè¿ç»­å˜é‡ï¼‰
      âœ“ ä¿ç•™ education-num ä¸è§„èŒƒåŒ–ï¼ˆåºæ•°åˆ†ç±»ç¼–ç ï¼‰
  
  Part 2 - PCA é™ç»´ï¼š
    å†³ç­–ï¼šâŒ æ”¾å¼ƒ PCA
    åŸå› ï¼š5ä¸ªç‰¹å¾æ–¹å·®åˆ†å¸ƒå‡åŒ€ï¼Œæ— æ³•æœ‰æ•ˆå‹ç¼©ç»´åº¦
  æ•°æ®è§„æ¨¡ï¼š48,169 è¡Œ Ã— 14 åˆ—ï¼ˆä¸å˜ï¼‰

é˜¶æ®µ4ï¼šç‰¹å¾æ„é€ ï¼ˆStep 4ï¼‰
  è¾“å…¥ï¼šnormalized_data.csv
  è¾“å‡ºï¼šfeature_constructed_data.csv
  æ“ä½œï¼š
    âœ“ æ„é€  3 ä¸ªäº¤äº’ç‰¹å¾ï¼š
      1. work_intensity = education-num Ã— hours-per-week
      2. net_capital = capital-gain - capital-loss
      3. work_age_ratio = hours-per-week / age
    âœ“ éªŒè¯ç‰¹å¾æœ‰æ•ˆæ€§ï¼ˆæŒ‰ income åˆ†ç»„å¯¹æ¯”ï¼‰
  æ•°æ®è§„æ¨¡ï¼š48,169 è¡Œ Ã— 14 åˆ— â†’ 48,169 è¡Œ Ã— 17 åˆ—ï¼ˆæ–°å¢ 3 åˆ—ï¼‰

é˜¶æ®µ5ï¼šç‹¬çƒ­ç¼–ç ï¼ˆStep 5 - å½“å‰é˜¶æ®µï¼‰
  è¾“å…¥ï¼šfeature_constructed_data.csv
  è¾“å‡ºï¼šfinal_preprocessed_data.csv
  æ“ä½œï¼š
    âœ“ å¯¹ 7 ä¸ªåˆ†ç±»å˜é‡è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼ˆdrop_first=Trueï¼‰
    âœ“ åˆ é™¤åŸåˆ†ç±»åˆ—ï¼š7 åˆ—
    âœ“ æ–°å¢ç‹¬çƒ­ç¼–ç åˆ—ï¼š76 åˆ—
  æ•°æ®è§„æ¨¡ï¼š48,169 è¡Œ Ã— 17 åˆ— â†’ 48,169 è¡Œ Ã— {df_encoded.shape[1]} åˆ—ï¼ˆæ–°å¢ {columns_added} åˆ—ï¼‰

================================================================================

ã€æœ€ç»ˆæ•°æ®ç‰¹å¾æ¸…å•ã€‘
--------------------------------------------------------------------------------

ç‰¹å¾ç±»å‹ç»Ÿè®¡ï¼š
  1. åŸå§‹æ•°å€¼å‹ç‰¹å¾ï¼š{len(original_numerical)} åˆ—
     - ageï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
     - fnlwgtï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
     - education-numï¼ˆæœªæ ‡å‡†åŒ–ï¼Œåºæ•°ç¼–ç ï¼‰
     - capital-gainï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
     - capital-lossï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
     - hours-per-weekï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
  
  2. æ–°æ„é€ ç‰¹å¾ï¼š3 åˆ—
     - work_intensityï¼ˆå·¥ä½œå¼ºåº¦ï¼Œæœªæ ‡å‡†åŒ–ï¼‰
     - net_capitalï¼ˆèµ„æœ¬å‡€æ”¶ç›Šï¼ŒåŸºäºå·²æ ‡å‡†åŒ–ç‰¹å¾ï¼‰
     - work_age_ratioï¼ˆå¹´é¾„å·¥ä½œæ¯”ï¼ŒåŸºäºå·²æ ‡å‡†åŒ–ç‰¹å¾ï¼‰
  
  3. ç‹¬çƒ­ç¼–ç ç‰¹å¾ï¼š{len(one_hot_cols)} åˆ—
     æ¥è‡ªä»¥ä¸‹åŸå§‹åˆ†ç±»å˜é‡ï¼š
     - workclassï¼ˆ8ç±» â†’ 7åˆ—ï¼‰
     - marital-statusï¼ˆ7ç±» â†’ 6åˆ—ï¼‰
     - occupationï¼ˆ14ç±» â†’ 13åˆ—ï¼‰
     - relationshipï¼ˆ6ç±» â†’ 5åˆ—ï¼‰
     - raceï¼ˆ5ç±» â†’ 4åˆ—ï¼‰
     - sexï¼ˆ2ç±» â†’ 1åˆ—ï¼‰
     - native-countryï¼ˆ41ç±» â†’ 40åˆ—ï¼‰
  
  4. ç›®æ ‡å˜é‡ï¼š1 åˆ—
     - incomeï¼ˆ>50K / <=50Kï¼‰

æ€»è®¡ï¼š{df_encoded.shape[1]} åˆ—ï¼ˆç‰¹å¾ {df_encoded.shape[1]-1} åˆ— + ç›®æ ‡ 1 åˆ—ï¼‰

================================================================================

ã€æ•°æ®è´¨é‡è¯„ä¼°ã€‘
--------------------------------------------------------------------------------

1. å®Œæ•´æ€§ï¼š
   âœ“ æ— ç¼ºå¤±å€¼ï¼ˆ{missing_count} ä¸ª NaNï¼‰
   âœ“ æ ·æœ¬å®Œæ•´ï¼ˆ{df_encoded.shape[0]:,} è¡Œï¼‰

2. ä¸€è‡´æ€§ï¼š
   âœ“ æ•°å€¼å‹ç‰¹å¾å·²è§„èŒƒåŒ–ï¼ˆé™¤ education-num å¤–ï¼‰
   âœ“ åˆ†ç±»ç‰¹å¾å·²è½¬æ¢ä¸ºæ•°å€¼æ ¼å¼ï¼ˆ0/1ï¼‰
   âœ“ ç›®æ ‡å˜é‡ä¿ç•™åŸå§‹æ ‡ç­¾ï¼ˆä¾¿äºè§£é‡Šï¼‰

3. è§„èŒƒæ€§ï¼š
   âœ“ æ‰€æœ‰ç‰¹å¾å¯ç›´æ¥ç”¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡
   âœ“ é¿å…äº†è™šæ‹Ÿå˜é‡é™·é˜±ï¼ˆdrop_first=Trueï¼‰
   âœ“ åˆ—å‘½åæ¸…æ™°ï¼ˆåŸå˜é‡å_ç±»åˆ«åï¼‰

4. è§„æ¨¡ï¼š
   åŸå§‹æ•°æ®ï¼š48,842 è¡Œ Ã— 15 åˆ—
   æœ€ç»ˆæ•°æ®ï¼š{df_encoded.shape[0]:,} è¡Œ Ã— {df_encoded.shape[1]} åˆ—
   æ ·æœ¬ä¿ç•™ç‡ï¼š{(df_encoded.shape[0] / 48842) * 100:.2f}%
   ç‰¹å¾æ‰©å±•ç‡ï¼š{((df_encoded.shape[1] - 15) / 15) * 100:.1f}%

================================================================================

ã€å…³é”®å†³ç­–æ€»ç»“ã€‘
--------------------------------------------------------------------------------

1. ç¼ºå¤±å€¼å¤„ç†ï¼š
   å†³ç­–ï¼šä¼—æ•°å¡«å……ï¼ˆåˆ†ç±»å˜é‡ï¼‰
   ç†ç”±ï¼šä¿ç•™æ›´å¤šæ ·æœ¬ï¼Œé¿å…ä¿¡æ¯æŸå¤±

2. ç¦»ç¾¤ç‚¹å¤„ç†ï¼š
   å†³ç­–ï¼šä»…åˆ é™¤ä¸šåŠ¡é€»è¾‘æ˜æ˜¾ä¸åˆç†çš„ç¦»ç¾¤ç‚¹
   ç†ç”±ï¼šcapital-gain/loss çš„æç«¯å€¼æ˜¯çœŸå®çš„é«˜æ”¶å…¥ç‰¹å¾

3. ç‰¹å¾åˆ é™¤ï¼š
   å†³ç­–ï¼šåˆ é™¤ education æ–‡æœ¬åˆ—ï¼Œä¿ç•™ education-num
   ç†ç”±ï¼š100% è¯­ä¹‰å†—ä½™ï¼Œæ•°å€¼ç¼–ç æ›´é€‚åˆå»ºæ¨¡

4. è§„èŒƒåŒ–æ–¹æ³•ï¼š
   å†³ç­–ï¼šZ-score æ ‡å‡†åŒ–
   ç†ç”±ï¼šæ•°æ®å­˜åœ¨æç«¯å€¼ï¼ŒZ-score æ›´é²æ£’

5. PCA é™ç»´ï¼š
   å†³ç­–ï¼šæ”¾å¼ƒ PCA
   ç†ç”±ï¼šæ— æ³•æœ‰æ•ˆå‹ç¼©ç»´åº¦ï¼Œä¸šåŠ¡è§£é‡Šæ€§å¼±

6. ç‰¹å¾æ„é€ ï¼š
   å†³ç­–ï¼šæ„é€  3 ä¸ªäº¤äº’ç‰¹å¾
   ç†ç”±ï¼šåŸºäºä¸šåŠ¡é€»è¾‘ï¼ŒéªŒè¯æœ‰æ•ˆ

7. ç‹¬çƒ­ç¼–ç ï¼š
   å†³ç­–ï¼šä½¿ç”¨ drop_first=True
   ç†ç”±ï¼šé¿å…å¤šé‡å…±çº¿æ€§ï¼Œå‡å°‘ç‰¹å¾æ•°

================================================================================

ã€åç»­å»ºè®®ã€‘
--------------------------------------------------------------------------------

1. æ¨¡å‹è®­ç»ƒï¼š
   âœ“ æ•°æ®å·²å®Œå…¨å‡†å¤‡å¥½ï¼Œå¯ç›´æ¥ç”¨äºå»ºæ¨¡
   âœ“ æ¨èæ¨¡å‹ï¼š
     - æ ‘æ¨¡å‹ï¼šéšæœºæ£®æ—ã€XGBoostã€LightGBMï¼ˆä¸å—ç‰¹å¾è§„æ¨¡å½±å“ï¼‰
     - çº¿æ€§æ¨¡å‹ï¼šé€»è¾‘å›å½’ã€SVMï¼ˆå·²è§„èŒƒåŒ–ï¼Œé€‚ç”¨ï¼‰
     - ç¥ç»ç½‘ç»œï¼šMLPï¼ˆå·²è§„èŒƒåŒ–ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼‰

2. ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰ï¼š
   - é«˜ç»´æ•°æ®ï¼ˆ{df_encoded.shape[1]}åˆ—ï¼‰å¯èƒ½å­˜åœ¨å†—ä½™
   - å¯ä»¥ä½¿ç”¨ç‰¹å¾é‡è¦æ€§ã€L1æ­£åˆ™åŒ–ç­‰æ–¹æ³•ç­›é€‰
   - å»ºè®®å…ˆè®­ç»ƒåŸºå‡†æ¨¡å‹ï¼Œå†æ ¹æ®éœ€è¦åšç‰¹å¾é€‰æ‹©

3. æ•°æ®åˆ’åˆ†ï¼š
   - è®­ç»ƒé›† / æµ‹è¯•é›†åˆ’åˆ†ï¼š80/20 æˆ– 70/30
   - è€ƒè™‘ä½¿ç”¨åˆ†å±‚æŠ½æ ·ï¼ˆstratified splitï¼‰ä¿æŒ income æ¯”ä¾‹
   - KæŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§

4. æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼š
   - å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰
   - ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1-score
   - ROCæ›²çº¿ã€AUCå€¼
   - æ··æ·†çŸ©é˜µ

5. å®éªŒå¯¹æ¯”ï¼š
   - å¯¹æ¯”ä½¿ç”¨æ–°ç‰¹å¾å‰åçš„æ¨¡å‹æ€§èƒ½
   - å¯¹æ¯”ä¸åŒè§„èŒƒåŒ–æ–¹æ³•çš„æ•ˆæœ
   - å¯¹æ¯”ç‰¹å¾é€‰æ‹©å‰åçš„æ€§èƒ½

================================================================================
"""

write_log(summary)
write_log("")

# ================================================================================
# ç¨‹åºç»“æŸ
# ================================================================================
write_log("=" * 80)
write_log("âœ… ç‹¬çƒ­ç¼–ç æ¨¡å—æ‰§è¡Œå®Œæˆ")
write_log("=" * 80)
write_log("")

print("\n" + "=" * 80)
print("âœ… ç‹¬çƒ­ç¼–ç ï¼ˆå“‘å˜é‡ç¼–ç ï¼‰å·²å…¨éƒ¨å®Œæˆï¼")
print("=" * 80)
print(f"\nğŸ“Š ç”Ÿæˆæ–‡ä»¶æ¸…å•ï¼š")
print(f"  1. final_preprocessed_data.csv           - æœ€ç»ˆé¢„å¤„ç†æ•°æ®")
print(f"  2. step5_one_hot_encoding_log.txt        - è¯¦ç»†æ—¥å¿—æ–‡ä»¶")
print(f"  3. å›¾9_ç‹¬çƒ­ç¼–ç å‰ååˆ—æ•°å¯¹æ¯”.png          - ç¼–ç å‰åå¯¹æ¯”å›¾")
print(f"  4. å›¾10_ç¼–ç åç‰¹å¾ç±»å‹åˆ†å¸ƒ.png           - ç‰¹å¾ç±»å‹åˆ†å¸ƒå›¾")
print(f"\nğŸ“ˆ æ ¸å¿ƒç»“æœï¼š")
print(f"  - ç¼–ç å‰ï¼š{df.shape[1]} åˆ—")
print(f"  - ç¼–ç åï¼š{df_encoded.shape[1]} åˆ—")
print(f"  - æ–°å¢ï¼š{columns_added} åˆ—ï¼ˆå¢é•¿ {(columns_added/df.shape[1])*100:.1f}%ï¼‰")
print(f"  - æ ·æœ¬æ•°ï¼š{df_encoded.shape[0]:,} è¡Œï¼ˆä¸å˜ï¼‰")
print(f"\nğŸ¯ æ•°æ®é¢„å¤„ç†å…¨æµç¨‹å·²å®Œæˆï¼")
print(f"  âœ“ æ•°æ®æ¸…æ´—")
print(f"  âœ“ æ•°æ®é›†æˆ")
print(f"  âœ“ æ•°æ®è§„çº¦ï¼ˆè§„èŒƒåŒ–ï¼‰")
print(f"  âœ“ ç‰¹å¾æ„é€ ")
print(f"  âœ“ ç‹¬çƒ­ç¼–ç ")
print(f"\nâœ“ æœ€ç»ˆæ•°æ®å¯ç›´æ¥ç”¨äºæœºå™¨å­¦ä¹ å»ºæ¨¡")
print("=" * 80 + "\n")

